Usage
=====

.. role:: python(code)
   :language: python

The idea of this package is simple: Just provide your data,
select a model (or provide your own), and get the predictions.

File Structure
--------------
The ``data_root`` directory should have the following structure:
::

  - gauge_info/
    - gauge_info.csv
  - discharge/
    - *.nc
  - forcings/
    - lumped/
      - *_<basinId>.rvt

``gauge_info.csv`` is a csv-file with at least a column 'Gauge_ID'
and possibly further static basin attributes.
If the file further contains a column 'Cal_Val' indicating calibration ('C')
and validation ('V') basins, the method :python:`datautils.get_basin_list()` can
be used to select calibration or validation basins.

Currently, only NetCDF files for discharge and rvt files for daily lumped forcings are supported.
The NetCDF discharge files have the variables ``station_id(nstations)``, ``time(time)``,
and ``Q(nstations, time)``.
If multiple NetCDF-files exist, their contents are merged and duplicate stations ignored.

The rvt-files are simple space-separated csv-files as generated by Raven.
An example rvt file looks like this:

::

  :MultiData
  2000-01-01 00:00:00 1.0 6210
  :Parameters,                    PRECIP,                 TEMP_DAILY_AVE,                 TEMP_MIN,                       TEMP_MAX
  :Units,                         mm/d,                           C,                              C,                              C
                          0.471473290000,                 -2.02099736340,                 -6.94564452600,                 3.092910019300,
  ...
  :EndMultiData

There are four header lines and one footer line; and the second header line starts with
the start date formatted as ``YYYY-MM-DD``. The third header line contains the column
names as ``:Parameters, <col1>, <col2>, ..., <colN>``.


Training
--------
For training, create an :python:`Experiment` with the path to the data folder, specify a run directory
to store results, start and end date, the training basins, and forcing and static basin attributes.
Then, set the model to use for training, and start training with :python:`exp.train()`:

.. code-block:: python

  data_path = Path('./data')
  run_dir = Path('./experiments')
  exp = Experiment(data_path, is_train=True, run_dir=run_dir,
                   start_date='01012000', end_date='31122015',
                   basins=['00AAA123', '00AAB234', '00AAC567'],
                   forcing_attributes=['precip', 'tmax', 'tmin'],
                   static_attributes=['area', 'regulation'])
  exp.set_model(model)
  exp.train()

Any model class that implements the methods :python:`train(self, ds: LumpedH5)` and
:python:`predict(self, ds: LumpedBasin)` can be used. Pre-implemented models can be found
in :python:`mlstream.models`.

Inference
---------
To run inference after training, create a new :python:`Experiment` with :python:`is_train = False`,
provide the data path, the path to the run directory from training, the test basins,
and start and end date.
Load and set the trained model (which was saved in the run directory during training),
and run predictions with :python:`exp.predict()`, which will return a DataFrame of predictions.

.. code-block:: python

  exp = Experiment(data_path, is_train=False, run_dir=run_dir,
                   basins=['01ABC123', '02DEF123'],
                   start_date='01012016', end_date='31122018')
  model.load(run_dir / 'model.pkl')
  exp.set_model(model)  
  results = exp.predict()

To obtain NSE scores for each test basin, run :python:`exp.get_nses()`.